{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoZrGgDOWf6M"
      },
      "source": [
        "<img src=https://audiovisuales.icesi.edu.co/assets/custom/images/ICESI_logo_prin_descriptor_RGB_POSITIVO_0924.jpg width=200>\n",
        "\n",
        "*Milton Orlando Sarria Paja, PhD.*\n",
        "\n",
        "----\n",
        "\n",
        "#### *cambios realizados a cuaderno*\n",
        "\n",
        "# üõ†Ô∏è Metodolog√≠as para validar un clasificador\n",
        "\n",
        "Ajustar los par√°metros de un modelo para realizar alg√∫n tipo de predicci√≥n y evaluar el sistema en los mismos datos que se han utilizado para el entrenamiento es un **error metodol√≥gico**.\n",
        "\n",
        "Un modelo que simplemente repite las etiquetas de los datos que acaba de ver durante el entrenamiento podr√≠a hacer un trabajo perfecto, pero ¬øqu√© pasa con los datos que **no ha visto**? ¬øEl resultado corresponder√° a algo √∫til? Esta situaci√≥n se conoce como **sobreajuste** o **overfitting**.\n",
        "\n",
        "Para evitar este tipo de problemas y tener una mejor idea del **comportamiento real** del sistema (ya sea de clasificaci√≥n o regresi√≥n) al procesar datos **desconocidos**, se **divide el conjunto de datos en dos subconjuntos**: `X_train` y `X_test`. Esto permite que el sistema automatizado se **entrene** usando los datos de `X_train` y se **eval√∫e** con los datos de `X_test`.\n",
        "\n",
        "---\n",
        "\n",
        "Sin embargo, todav√≠a existe el riesgo de que el modelo se **sobreentrene en el conjunto de prueba**, ya que los par√°metros pueden ajustarse una y otra vez hasta que el estimador alcance un rendimiento √≥ptimo en ese conjunto. De esta forma, el conocimiento sobre el conjunto de prueba puede ‚Äú**filtrarse**‚Äù en el modelo, y las m√©tricas de evaluaci√≥n dejan de reflejar el **rendimiento de generalizaci√≥n**.\n",
        "\n",
        "Para resolver este problema, se puede reservar **otra parte adicional** del conjunto de datos como un llamado **‚Äúconjunto de validaci√≥n‚Äù**. El proceso es el siguiente:\n",
        "1. El entrenamiento se realiza sobre el conjunto de entrenamiento (`train`).\n",
        "2. La evaluaci√≥n intermedia se lleva a cabo en el conjunto de validaci√≥n (`validation`).\n",
        "3. Cuando el experimento parece exitoso, se realiza la evaluaci√≥n **final** en el conjunto de prueba (`test`).\n",
        "\n",
        "---\n",
        "\n",
        "No obstante, al **dividir los datos disponibles en tres subconjuntos**, se reduce de manera significativa la cantidad de muestras que pueden utilizarse para **aprender el modelo**, y los resultados pueden depender mucho de una **elecci√≥n aleatoria particular** de los subconjuntos de entrenamiento y validaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "Una soluci√≥n a este problema es un procedimiento llamado **validaci√≥n cruzada** (o **cross-validation**, abreviado **CV**).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vANs8PVwWf6Q"
      },
      "source": [
        "Hay diferentes estrategias; utilizaremos las herramientas disponibles en scikit-learn para este prop√≥sito.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/cross_validation.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLH-BpOkWf6R"
      },
      "source": [
        "## Using the IRIS database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP7qCuxvWf6S"
      },
      "outputs": [],
      "source": [
        "#load tools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpXH1V2jWf6S"
      },
      "outputs": [],
      "source": [
        "##Load iris\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar el dataset Iris\n",
        "iris = load_iris()\n",
        "\n",
        "# Convertirlo en un DataFrame para mejor visualizaci√≥n\n",
        "df_iris = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "\n",
        "# Agregar la columna de la clase (target)\n",
        "df_iris['target'] = iris.target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFKQDq5LWf6T",
        "outputId": "6e161042-c533-4b11-b34b-d9ac71f0c071"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              "0                5.1               3.5                1.4               0.2   \n",
              "1                4.9               3.0                1.4               0.2   \n",
              "2                4.7               3.2                1.3               0.2   \n",
              "3                4.6               3.1                1.5               0.2   \n",
              "4                5.0               3.6                1.4               0.2   \n",
              "\n",
              "   target  \n",
              "0       0  \n",
              "1       0  \n",
              "2       0  \n",
              "3       0  \n",
              "4       0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Verify data\n",
        "df_iris.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAtnvJjfWf6T",
        "outputId": "399023cb-4fc3-4bae-ad0e-dd09f946081e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "145    2\n",
              "146    2\n",
              "147    2\n",
              "148    2\n",
              "149    2\n",
              "Name: target, Length: 150, dtype: int32"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_iris['target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUt4e48MWf6U"
      },
      "source": [
        "## K-Fold  \n",
        "`KFold` divide todas las muestras en **$k$ grupos de muestras**, llamados **folds** (si $k = n$, esto es equivalente a la estrategia de **Leave One Out**), de tama√±os iguales (si es posible).  \n",
        "La funci√≥n de predicci√≥n se aprende utilizando **$k - 1$ folds**, y el fold que se deja fuera se utiliza para **prueba**.\n",
        "\n",
        "\n",
        "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_006.png\" width=\"450\">\n",
        "\n",
        "There is a problem....."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfSrjEvKWf6U",
        "outputId": "8c5acec4-7563-4157-d71b-8b7552418d3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(150, 4)\n",
            "(150,)\n",
            "test labels:  [0 0 0 0 0 1 1 1 2 2 2 2 2 2 2]\n",
            "test labels:  [0 0 0 0 0 0 0 1 1 1 1 2 2 2 2]\n",
            "test labels:  [0 0 0 0 1 1 1 1 1 1 2 2 2 2 2]\n",
            "test labels:  [0 0 0 1 1 1 1 1 1 2 2 2 2 2 2]\n",
            "test labels:  [0 0 0 0 0 0 1 1 1 1 1 1 1 1 2]\n",
            "test labels:  [0 0 0 1 1 1 1 2 2 2 2 2 2 2 2]\n",
            "test labels:  [0 0 0 0 0 0 0 1 1 1 1 1 2 2 2]\n",
            "test labels:  [0 0 0 0 0 1 1 1 2 2 2 2 2 2 2]\n",
            "test labels:  [0 0 0 1 1 1 1 1 2 2 2 2 2 2 2]\n",
            "test labels:  [0 0 0 0 0 0 0 1 1 1 1 1 1 2 2]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "X = np.array(df_iris.drop(columns=['target']))\n",
        "Y = np.array(df_iris['target'])\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "kf = KFold(n_splits=10,shuffle=True)\n",
        "for train_index, test_index in kf.split(X):\n",
        "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = Y[train_index], Y[test_index]\n",
        "        print(\"test labels: \", y_test)\n",
        "        #print(\"train labels: \", y_train)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCcsk6HaWf6U"
      },
      "source": [
        "\n",
        "**Para resolver el problema, podemos mezclar (shuffle) el conjunto de datos antes de aplicar el algoritmo anterior. Usamos una permutaci√≥n aleatoria de los √≠ndices de la siguiente manera:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsWWYSPZWf6V",
        "outputId": "6a5e95f2-b452-44a4-fd1e-e4cc57918adc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valores de x:               [10 11 12 13 14 15 16 17 18 19]\n",
            "nuevos indices para x:      [4 3 6 5 1 9 0 7 8 2]\n",
            "valores de x, desordenados: [14 13 16 15 11 19 10 17 18 12]\n"
          ]
        }
      ],
      "source": [
        "#values from 10 to 20\n",
        "x=np.arange(10,20)\n",
        "print(\"valores de x:              \", x)\n",
        "#shufle\n",
        "y=np.random.permutation(10)\n",
        "print(\"nuevos indices para x:     \", y)\n",
        "\n",
        "#use the new indexes\n",
        "x=x[y]\n",
        "print(\"valores de x, desordenados:\", x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5V3FBfNWf6V"
      },
      "outputs": [],
      "source": [
        "#Do the same for the IRIS dataset\n",
        "ind=np.random.permutation(Y.size)\n",
        "\n",
        "X=X[ind,:]\n",
        "Y=Y[ind]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDA9BjelWf6V",
        "outputId": "525aa9c7-b3fb-4574-ac77-741cd6583054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test labels:  [2 1 2 1 0 2 0 0 2 2 0 0 1 1 2]\n",
            "test labels:  [0 1 1 2 0 0 1 1 0 1 2 1 1 2 1]\n",
            "test labels:  [2 2 2 2 0 0 2 2 2 1 1 0 0 0 2]\n",
            "test labels:  [0 2 1 1 0 1 1 1 2 2 0 1 1 0 2]\n",
            "test labels:  [0 2 2 2 2 0 0 2 2 2 0 1 0 2 1]\n",
            "test labels:  [2 2 2 0 0 0 1 1 1 1 2 0 0 0 2]\n",
            "test labels:  [2 1 0 1 2 2 1 2 2 1 0 1 1 2 2]\n",
            "test labels:  [0 1 1 1 0 0 1 1 2 1 1 0 1 1 2]\n",
            "test labels:  [2 0 1 0 2 0 2 1 2 0 2 0 1 0 1]\n",
            "test labels:  [0 0 2 0 0 1 0 0 1 0 1 0 0 2 1]\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=10)\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = Y[train_index], Y[test_index]\n",
        "        print(\"test labels: \", y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b3sd1qQWf6V"
      },
      "source": [
        "Hagamos un experimento\n",
        "\n",
        "1. Logistic regression\n",
        "2. KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDLbXAW3Wf6W",
        "outputId": "c0be870b-2ee6-478b-e54f-41ebd61ab1ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression: average = 96.000000, std = 6.798693\n",
            "KNN                : average = 96.000000, std = 5.333333\n"
          ]
        }
      ],
      "source": [
        "#generamos dos vectores de ceros para guardar la tasa de acierto (% de muestras clasificadas correctamente) de\n",
        "#los dos clasificadors, uno pada caso\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#numero de folds\n",
        "k=10\n",
        "\n",
        "acc1 = []\n",
        "acc2 = []\n",
        "\n",
        "kf = KFold(n_splits=k)\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = Y[train_index], Y[test_index]\n",
        "        #CLF 1\n",
        "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "        clf1.fit(X_train, y_train)\n",
        "        #evaluate\n",
        "        yp1 = clf1.predict(X_test)\n",
        "        acc1.append(np.sum(yp1==y_test)/y_test.size*100)\n",
        "\n",
        "        #Clf2\n",
        "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
        "        clf2.fit(X_train, y_train)\n",
        "\n",
        "        #Evaluate\n",
        "        yp2 = clf2.predict(X_test)\n",
        "        acc2.append(np.sum(yp2==y_test)/y_test.size*100)\n",
        "\n",
        "acc1=np.array(acc1)\n",
        "acc2=np.array(acc2)\n",
        "\n",
        "\n",
        "print(\"Logistic regression: average = %f, std = %f\"% (acc1.mean(), acc1.std()))\n",
        "print(\"KNN                : average = %f, std = %f\"% (acc2.mean(), acc2.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or8Wj8bxWf6W"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrFfPslAWf6W"
      },
      "source": [
        "## Leave One Out (LOO)\n",
        "\n",
        "**LeaveOneOut** (o **LOO**) es una t√©cnica simple de **validaci√≥n cruzada**.  \n",
        "Cada conjunto de entrenamiento se crea tomando **todas las muestras excepto una**, siendo el conjunto de prueba la **muestra que se deja fuera**.  \n",
        "As√≠, para **$n$ muestras**, tenemos **$n$ conjuntos de entrenamiento diferentes** y **$n$ conjuntos de prueba diferentes**.  \n",
        "Este procedimiento de validaci√≥n cruzada **no desperdicia muchos datos**, ya que solo se **elimina una muestra** del conjunto de entrenamiento:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzc6YBi9Wf6W",
        "outputId": "d3c47b9b-a9cc-486e-caa5-b2c0a770fd50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(150, 4)\n",
            "(150,)\n",
            "Logistic regression: accuracy =  96.66666666666667\n",
            "KNN                : accuracy =  96.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "acc1 = []\n",
        "acc2 = []\n",
        "\n",
        "X = np.array(df_iris.drop(columns=['target']))\n",
        "Y = np.array(df_iris['target'])\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "#index generator\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "for train, test in loo.split(X):\n",
        "        X_train, X_test = X[train], X[test]\n",
        "        y_train, y_test = Y[train], Y[test]\n",
        "        #Clf 1\n",
        "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "        clf1.fit(X_train, y_train)\n",
        "        #eval\n",
        "        yp1 = clf1.predict(X_test)\n",
        "        acc1.append(yp1==y_test)\n",
        "\n",
        "        #clf2\n",
        "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
        "        clf2.fit(X_train, y_train)\n",
        "\n",
        "        #eval\n",
        "        yp2 = clf2.predict(X_test)\n",
        "        acc2.append(yp2==y_test)\n",
        "\n",
        "acc1=np.array(acc1).sum()/len(acc1)*100\n",
        "acc2=np.array(acc2).sum()/len(acc2)*100\n",
        "\n",
        "\n",
        "\n",
        "print(\"Logistic regression: accuracy = \", acc1)\n",
        "print(\"KNN                : accuracy = \", acc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtqM8goBWf6X"
      },
      "source": [
        "Se puede notar que la tasa de acierto es igual a la que se obtuvo con KFOLDS, sin embargo en este caso no es posible calcular un promedio o una desviaci√≥n estandard, pues en cada iteraci√≥n solo habia una muestra, por lo que el acierto es 100% si esa muestra se clasifica bien, o 0% si se clasifica mal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBb9EFHLWf6X"
      },
      "source": [
        "## Validaci√≥n cruzada aleatoria = Shuffle & Split\n",
        "\n",
        "El iterador **ShuffleSplit** generar√° un n√∫mero definido por el usuario de divisiones **independientes** del conjunto de datos en **entrenamiento** y **prueba**.  \n",
        "Primero se mezclan (shuffle) las muestras y luego se dividen en un par de conjuntos: uno de entrenamiento y otro de prueba.\n",
        "\n",
        "Es posible **controlar la aleatoriedad** para obtener resultados **reproducibles**, estableciendo expl√≠citamente la semilla en el generador de n√∫meros pseudoaleatorios mediante el par√°metro `random_state`.\n",
        "\n",
        "```python\n",
        "ShuffleSplit(n_splits=20, test_size=0.3, random_state=0)\n",
        "```\n",
        "\n",
        "- **n_splits**: n√∫mero de divisiones a realizar.  \n",
        "- **test_size**: proporci√≥n de los datos que se usar√° para prueba.  \n",
        "\n",
        "Por ejemplo: **70% - 30%** (entrenamiento - prueba).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnLJMEIyWf6X",
        "outputId": "f959a114-d28b-4300-e9f7-64e27c6db15f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(150, 4)\n",
            "(150,)\n",
            "Logistic regression: Average = 96.666667, std = 3.022549\n",
            "KNN                : Average = 96.000000, std = 3.265986\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "acc1 = []\n",
        "acc2 = []\n",
        "\n",
        "X = np.array(df_iris.drop(columns=['target']))\n",
        "Y = np.array(df_iris['target'])\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "\n",
        "#\n",
        "ss = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n",
        "\n",
        "for train_index, test_index in ss.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = Y[train_index], Y[test_index]\n",
        "        #clf1\n",
        "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "        clf1.fit(X_train, y_train)\n",
        "        #evaluate\n",
        "        yp1 = clf1.predict(X_test)\n",
        "        acc1.append(np.sum(yp1==y_test)/y_test.size*100)\n",
        "\n",
        "        #clf2\n",
        "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
        "        clf2.fit(X_train, y_train)\n",
        "\n",
        "        #Evaluate\n",
        "        yp2 = clf2.predict(X_test)\n",
        "        acc2.append(np.sum(yp2==y_test)/y_test.size*100)\n",
        "\n",
        "\n",
        "acc1=np.array(acc1)\n",
        "acc2=np.array(acc2)\n",
        "\n",
        "print(\"Logistic regression: Average = %f, std = %f\"% (acc1.mean(), acc1.std()))\n",
        "print(\"KNN                : Average = %f, std = %f\"% (acc2.mean(), acc2.std()))\n",
        "del X, Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyiBZThBWf6Y"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgtJ7qWiWf6Y"
      },
      "source": [
        "## Iteradores de validaci√≥n cruzada con estratificaci√≥n basada en las etiquetas de clase.\n",
        "\n",
        "Algunos problemas de clasificaci√≥n pueden presentar un **gran desbalance** en la distribuci√≥n de las clases objetivo; por ejemplo, podr√≠a haber **muchas m√°s muestras negativas que positivas**.  \n",
        "En estos casos, se recomienda utilizar **muestreo estratificado**, como el que se implementa en **StratifiedKFold** y **StratifiedShuffleSplit**, para asegurarse de que las **frecuencias relativas de cada clase se mantengan aproximadamente iguales** en cada fold de entrenamiento y validaci√≥n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFZf3ym7Wf6Y"
      },
      "source": [
        "### Stratified k-fold\n",
        "\n",
        "\n",
        "**StratifiedKFold** es una variaci√≥n de **k-fold** que devuelve folds **estratificados**: cada conjunto contiene aproximadamente el **mismo porcentaje de muestras de cada clase objetivo** que el conjunto completo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqC3XYZ6Wf6Y",
        "outputId": "b07f9c26-54d7-4d5f-9066-24c2c64ea186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensi√≥n de X: (768, 8)\n",
            "Dimensi√≥n de y: (768,)\n",
            "class\n",
            "0    500\n",
            "1    268\n",
            "Name: count, dtype: int64\n",
            "0    1\n",
            "1    0\n",
            "2    1\n",
            "3    0\n",
            "4    1\n",
            "Name: class, dtype: category\n",
            "Categories (2, int64): [0, 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "diabetes_data = fetch_openml(name='diabetes', version=1, as_frame=True)\n",
        "df_diabetes = diabetes_data.frame\n",
        "\n",
        "#  Separar X (features) e Y (target)\n",
        "X = df_diabetes.drop(columns=['class'])  # Las caracter√≠sticas\n",
        "y = df_diabetes['class']                 # La etiqueta objetivo (diabetes: tested_positive / tested_negative)\n",
        "\n",
        "# Mostrar dimensiones de los datos\n",
        "print(f\"Dimensi√≥n de X: {X.shape}\")\n",
        "print(f\"Dimensi√≥n de y: {y.shape}\")\n",
        "\n",
        "y_numeric = y.map({'tested_negative': 0, 'tested_positive': 1})\n",
        "\n",
        "# Verificaci√≥n\n",
        "print(y_numeric.value_counts())\n",
        "print(y_numeric.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHrFh3I-Wf6Z",
        "outputId": "6516a098-8ee7-42d0-a275-22cec73e1277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(768, 8)\n",
            "(768,)\n",
            "Comparaci√≥n de rendimiento de los dos clasificadores:\n",
            "\n",
            "Logistic regression: promedio = 77.347915, std = 3.574822\n",
            "KNN                : promedio = 70.305878, std = 3.763358\n"
          ]
        }
      ],
      "source": [
        "#Stratified k-fold\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "\n",
        "X = np.array(df_diabetes.drop(columns=['class']) )\n",
        "Y = y_numeric\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "\n",
        "acc1 = []\n",
        "acc2 = []\n",
        "\n",
        "#se genera el generador de indices de forma estratificada\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "\n",
        "for train_index, test_index in skf.split(X,Y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = Y[train_index], Y[test_index]\n",
        "        #clasficador 1\n",
        "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "        clf1.fit(X_train, y_train)\n",
        "        #evaluar clf1 y guardar el resultado en acc1\n",
        "        yp1 = clf1.predict(X_test)\n",
        "        acc1.append(np.sum(yp1==y_test)/y_test.size*100)\n",
        "\n",
        "        #clasificador 2\n",
        "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
        "        clf2.fit(X_train, y_train)\n",
        "\n",
        "        #evaluar clf2 y guardar el resultado en acc2\n",
        "        yp2 = clf2.predict(X_test)\n",
        "        acc2.append(np.sum(yp2==y_test)/y_test.size*100)\n",
        "\n",
        "acc1=np.array(acc1)\n",
        "acc2=np.array(acc2)\n",
        "\n",
        "print(\"Comparaci√≥n de rendimiento de los dos clasificadores:\\n\")\n",
        "print(\"Logistic regression: promedio = %f, std = %f\"% (acc1.mean(), acc1.std()))\n",
        "print(\"KNN                : promedio = %f, std = %f\"% (acc2.mean(), acc2.std()))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6yhBOWHWf6Z"
      },
      "source": [
        "### Stratified Shuffle Split\n",
        "\n",
        "**StratifiedShuffleSplit** es una variaci√≥n de **ShuffleSplit**, que devuelve divisiones **estratificadas**, es decir, crea particiones **preservando el mismo porcentaje de cada clase objetivo** que hay en el conjunto completo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqfFhzQGWf6Z",
        "outputId": "d5a7d4e7-069e-4834-a5db-0dd1c32ec65c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(768, 8)\n",
            "(768,)\n",
            "Comparaci√≥n de rendimiento de los dos clasificadores:\n",
            "\n",
            "Logistic regression: promedio = 76.727273, std = 2.273548\n",
            "KNN                : promedio = 69.774892, std = 2.221231\n"
          ]
        }
      ],
      "source": [
        "#Stratified ShuffleSplit\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "\n",
        "X = np.array(df_diabetes.drop(columns=['class']) )\n",
        "Y = y_numeric\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "\n",
        "acc1 = []\n",
        "acc2 = []\n",
        "\n",
        "#se genera el generador de indices de forma estratificada\n",
        "sss = StratifiedShuffleSplit(n_splits=100,test_size=0.3)\n",
        "\n",
        "for train_index, test_index in sss.split(X,Y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = Y[train_index], Y[test_index]\n",
        "        #clasficador 1\n",
        "        clf1 = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "        clf1.fit(X_train, y_train)\n",
        "        #evaluar clf1 y guardar el resultado en acc1\n",
        "        yp1 = clf1.predict(X_test)\n",
        "        acc1.append(np.sum(yp1==y_test)/y_test.size*100)\n",
        "\n",
        "        #clasificador 2\n",
        "        clf2 = KNeighborsClassifier(n_neighbors=3)\n",
        "        clf2.fit(X_train, y_train)\n",
        "\n",
        "        #evaluar clf2 y guardar el resultado en acc2\n",
        "        yp2 = clf2.predict(X_test)\n",
        "        acc2.append(np.sum(yp2==y_test)/y_test.size*100)\n",
        "\n",
        "acc1=np.array(acc1)\n",
        "acc2=np.array(acc2)\n",
        "\n",
        "print(\"Comparaci√≥n de rendimiento de los dos clasificadores:\\n\")\n",
        "print(\"Logistic regression: promedio = %f, std = %f\"% (acc1.mean(), acc1.std()))\n",
        "print(\"KNN                : promedio = %f, std = %f\"% (acc2.mean(), acc2.std()))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlZzkmmAWf6Z"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNWUa7VLWf6a"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}